{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 4장 1번\n",
        "* 수백만 개의 특성을 가진 훈련세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?\n",
        "\n",
        "-> 확률적 경사 하강법을 사용하는 것이 적합하다. 그 이유는 한번에 전체 데이터를 사용하지 않고 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 가중치를 업데이트하는 방식이기에 수백만 개의 특성을 가진(큰 데이터셋을 가진) 훈련세트에서도 메모리 효율적으로 학습이 가능하기 때문이다.\n"
      ],
      "metadata": {
        "id": "l-SJWlC0ovCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4장 2번\n",
        "\n",
        "* 훈련 세트에 있는 특성들이 서로 다른 스케일을 가지고 있습니다. 이런 데이터에 잘 작동하지 않는 알고리즘은? 이 문제는 어떻게 해결해야할까?\n",
        "\n",
        "-> 스케일이 다르다는 것은 특성들의 값의 범위(단위나 크기 등)의 차이를 의미한다.\n",
        "\n",
        "-> 거리 기반으로 하는 선형 회귀, 로지스틱 회귀 등이 영향을 많이 받는다. 표준화를 사용하여 해결할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "-5U23y8Apo0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4장 3번\n",
        "\n",
        "* 경사 하강법으로 로지스틱 회귀 모델을 훈련시킬 때 지역 최솟값에 갇힐 가능성이 있을까요?\n",
        "\n",
        "-> 로지스틱 회귀 모델의 비용함수는 볼록 함수이므로 경사 하강법이 훈련될 때 지역 최솟값에 갇힐 가능성이 없다."
      ],
      "metadata": {
        "id": "LdyJ5MJtrCvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4장 12번\n",
        "사이킷런을 사용하지 않고 넘파이만 사용하여 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요. 이를 붓꽃 데이터셋 같은 분류 작업에 사용해보세요.\n",
        "\n",
        "### Softmax Regression (소프트맥스 회귀)란?\n",
        "다중 클래스 분류 문제(예: 붓꽃 데이터셋)에서 사용되는 확률 기반 모델.\n",
        "\n",
        "입력 데이터가 주어졌을 때, 각 클래스(예: Setosa, Versicolor, Virginica)에 속할 확률을 출력.\n",
        "\n",
        "\n",
        "\n",
        "데이터셋의 y값이 클래스 인덱스 (0,1,2)로 되어 있어서 소프트맥스 회귀 모델을 학습하기 위해 원-핫 벡터 형식으로 변환해야함."
      ],
      "metadata": {
        "id": "KCZ6BxfMjLxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85wz7FxfofwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "\n",
        "# DataFrame에서 꽃잎 길이와 너비 선택 후 NumPy 배열로 변환\n",
        "X = iris[\"data\"].iloc[:, [2, 3]].values\n",
        "y = iris[\"target\"].values  # NumPy 배열로 변환\n",
        "\n",
        "# 편향 항 추가 -> 1로 채워진 열벡터를 생성(첫 번째 열이 항상 1인 편향 항)\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
        "\n",
        "# 난수 시드 설정\n",
        "np.random.seed(2042)"
      ],
      "metadata": {
        "id": "mcsGK72FjLLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2          # 테스트 세트 비율 (20%)\n",
        "validation_ratio = 0.2     # 검증 세트 비율 (20%)\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)  # 테스트 데이터 개수\n",
        "validation_size = int(total_size * validation_ratio)  # 검증 데이터 개수\n",
        "train_size = total_size - test_size - validation_size  # 훈련 데이터 개수"
      ],
      "metadata": {
        "id": "nZ8HlsFqlC7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_indices = np.random.permutation(total_size)  # NumPy 배열 유지\n",
        "\n",
        "# 데이터셋 분할\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]\n"
      ],
      "metadata": {
        "id": "boEzghjslKsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot"
      ],
      "metadata": {
        "id": "y7DELoJ_lcS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train[:10]"
      ],
      "metadata": {
        "id": "j6wXuwYXmGf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "to_one_hot(y_train[:10])"
      ],
      "metadata": {
        "id": "OznlJKVtmI8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "metadata": {
        "id": "9GNrkkcLmLD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ],
      "metadata": {
        "id": "yDGxAEkamMqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = X_train.shape[1] # == 3 (2 features plus the bias term)\n",
        "n_outputs = len(np.unique(y_train))   # == 3 (3 iris classes)\n",
        "\n",
        "\n",
        "eta = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    if iteration % 500 == 0:\n",
        "        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "    Theta = Theta - eta * gradients\n"
      ],
      "metadata": {
        "id": "yW5OxqW9mNW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Theta"
      ],
      "metadata": {
        "id": "0Rxu_a9jmTIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "Elq9_uMvmUsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # 규제 하이퍼파라미터\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    if iteration % 500 == 0:\n",
        "        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "        loss = xentropy_loss + alpha * l2_loss\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "id": "gqb5dSsKmXgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "pzrm6g5hmeUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # 규제 하이퍼파라미터\n",
        "best_loss = np.inf\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    logits = X_valid.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "    else:\n",
        "        print(iteration - 1, best_loss)\n",
        "        print(iteration, loss, \"early stopping!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "Hx93OZIpmfTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "VCD5N5RWmhxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 다른 방법으로 코드 구현\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 붓꽃 데이터 로드 및 전처리\n",
        "iris = load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이(petal length)와 너비(petal width)만 사용\n",
        "y = iris[\"target\"]  # 클래스 레이블 (0: Setosa, 1: Versicolor, 2: Virginica)\n",
        "\n",
        "# 편향 항 추가 (Bias term, x0 = 1 추가)\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
        "\n",
        "# 원-핫 인코딩 (One-hot encoding)\n",
        "n_classes = np.max(y) + 1  # 3개 클래스 (0, 1, 2)\n",
        "y_one_hot = np.eye(n_classes)[y]  # 각 클래스를 원-핫 벡터로 변환\n",
        "\n",
        "# 데이터 분할 (훈련: 60%, 검증: 20%, 테스트: 20%)\n",
        "np.random.seed(2042)\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)  # 데이터 섞기\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train_one_hot = y_one_hot[rnd_indices[:train_size]]\n",
        "\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid_one_hot = y_one_hot[rnd_indices[train_size:-test_size]]\n",
        "\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test_one_hot = y_one_hot[rnd_indices[-test_size:]]\n",
        "\n",
        "# 소프트맥스 함수 구현\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # 오버플로우 방지\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "# 학습 설정\n",
        "n_inputs = X_train.shape[1]  # 입력 특성 개수 (2 + 1 = 3)\n",
        "n_outputs = n_classes  # 출력 클래스 개수 (3)\n",
        "\n",
        "# 가중치 초기화\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "# 학습 하이퍼파라미터\n",
        "eta = 0.1  # 학습률\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7  # log 계산 시 0 방지\n",
        "alpha = 0.1  # L2 정규화 하이퍼파라미터\n",
        "best_loss = np.inf  # 최적 손실값\n",
        "\n",
        "# 배치 경사 하강법 (Batch Gradient Descent) + 조기 종료\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)  # 선형 회귀 계산\n",
        "    Y_proba = softmax(logits)  # 소프트맥스 적용\n",
        "    error = Y_proba - y_train_one_hot  # 예측값 - 실제값\n",
        "\n",
        "    # 그래디언트 계산 (L2 정규화 포함)\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "\n",
        "    # 가중치 업데이트\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    # 검증 세트에서 손실 계산\n",
        "    logits_valid = X_valid.dot(Theta)\n",
        "    Y_proba_valid = softmax(logits_valid)\n",
        "    xentropy_loss = -np.mean(np.sum(y_valid_one_hot * np.log(Y_proba_valid + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss  # 총 손실\n",
        "\n",
        "    if iteration % 500 == 0:\n",
        "        print(f\"Iteration {iteration}: Loss = {loss:.5f}\")\n",
        "\n",
        "    # 조기 종료 (Early Stopping)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss  # 최적 손실 업데이트\n",
        "    else:\n",
        "        print(f\"Iteration {iteration-1}: Best Loss = {best_loss:.5f}\")\n",
        "        print(f\"Iteration {iteration}: Loss = {loss:.5f} (Early Stopping!)\")\n",
        "        break  # 손실이 증가하면 학습 종료\n",
        "\n",
        "# 최종 모델 평가\n",
        "logits_test = X_test.dot(Theta)\n",
        "Y_proba_test = softmax(logits_test)\n",
        "y_pred = np.argmax(Y_proba_test, axis=1)  # 확률이 가장 높은 클래스 선택\n",
        "y_test_labels = np.argmax(y_test_one_hot, axis=1)  # 실제 정답\n",
        "\n",
        "accuracy = np.mean(y_pred == y_test_labels)  # 정확도 계산\n",
        "print(f\"\\n테스트 정확도: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "HFHRqgAPog-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}